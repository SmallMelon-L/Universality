{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home1/jxwang/anaconda3/envs/mamba-sae/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Any\n",
    "from einops import rearrange\n",
    "from fancy_einsum import einsum\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from mamba_lens import HookedMamba\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = HookedMamba.from_pretrained(\"state-spaces/mamba-130m\", device=device)\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "vocab = tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [01:22<00:00,  2.59s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "N = 128 // batch_size\n",
    "seq_len = 1 + 128\n",
    "\n",
    "specific_patch_layer = 17\n",
    "\n",
    "names_filter = []\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    names_filter.append(f'blocks.{layer}.hook_h.{0}')\n",
    "    names_filter.append(f'blocks.{layer}.hook_h.{seq_len - 1}')\n",
    "    names_filter.append(f'blocks.{layer}.hook_A_bar')\n",
    "    names_filter.append(f'blocks.{layer}.hook_B_bar')\n",
    "    names_filter.append(f'blocks.{layer}.hook_ssm_input')\n",
    "    \n",
    "    \n",
    "patch_names_filter = []\n",
    "for layer in [specific_patch_layer]:\n",
    "    patch_names_filter.append(f'blocks.{layer}.hook_A_bar')\n",
    "    patch_names_filter.append(f'blocks.{layer}.hook_B_bar')\n",
    "    patch_names_filter.append(f'blocks.{layer}.hook_ssm_input')\n",
    "    # patch_names_filter.append(f'blocks.{layer}.hook_h.{0}')\n",
    "    # patch_names_filter.append(f'blocks.{layer}.hook_h.{2}')\n",
    "    # patch_names_filter.append(f'blocks.{layer}.hook_h.{3}')\n",
    "    # patch_names_filter.append(f'blocks.{layer}.hook_h.{4}')\n",
    "\n",
    "sum_real_logit_new = torch.zeros([model.cfg.d_conv]).to(device)\n",
    "sum_fake_logit_new = torch.zeros([model.cfg.d_conv]).to(device)\n",
    "sum_real_logit_orig = torch.zeros([model.cfg.d_conv]).to(device)\n",
    "sum_fake_logit_orig = torch.zeros([model.cfg.d_conv]).to(device)\n",
    "\n",
    "for batch in tqdm(range(N)):\n",
    "    random_input = torch.randint(0, len(vocab), size=(batch_size, seq_len)).to(device)\n",
    "    first_A_index = 0\n",
    "    B_index = 1\n",
    "    second_A_index = seq_len - 1\n",
    "\n",
    "    A_values = random_input[:, first_A_index]\n",
    "    B_values = random_input[:, B_index]\n",
    "    patch_B_values = torch.randint(0, len(vocab), size=(batch_size,)).to(device)\n",
    "    induction_input = random_input.clone()\n",
    "    induction_input[:, second_A_index] = A_values\n",
    "    patch_input = induction_input.clone()\n",
    "    patch_input[:, B_index] = patch_B_values\n",
    "    \n",
    "    logits, cache = model.run_with_cache(induction_input, names_filter=names_filter, fast_ssm=False, fast_conv=True, warn_disabled_hooks=False)\n",
    "    _, patch_cache = model.run_with_cache(patch_input, names_filter=patch_names_filter, fast_ssm=False, fast_conv=True, warn_disabled_hooks=False)\n",
    "    real_logit_orig = logits[:, second_A_index][torch.arange(logits.shape[0]), B_values]\n",
    "    fake_logit_orig = logits[:, second_A_index][torch.arange(logits.shape[0]), patch_B_values]\n",
    "    sum_real_logit_orig += real_logit_orig.sum()\n",
    "    sum_fake_logit_orig += fake_logit_orig.sum()\n",
    "    def generate_replacement_hook(layer, patch_layer, cache, patch_cache, pos):\n",
    "        if layer == patch_layer:\n",
    "            def replacement_hook(activations: torch.Tensor, hook: Any):\n",
    "                activations = torch.zeros_like(cache[f'blocks.{layer}.hook_h.{0}'])\n",
    "                for p in range(0, seq_len):\n",
    "                    if p != pos:\n",
    "                        activations = activations * cache[f'blocks.{layer}.hook_A_bar'][:,p,:,:] + cache[f'blocks.{layer}.hook_B_bar'][:,p,:,:] * cache[f'blocks.{layer}.hook_ssm_input'][:,p].view(batch_size, model.cfg.d_inner, 1)\n",
    "                    else:\n",
    "                        activations = activations * patch_cache[f'blocks.{layer}.hook_A_bar'][:,p+1,:,:] + patch_cache[f'blocks.{layer}.hook_B_bar'][:,p,:,:] * patch_cache[f'blocks.{layer}.hook_ssm_input'][:,p].view(batch_size, model.cfg.d_inner, 1)\n",
    "                return activations\n",
    "        else:\n",
    "            def replacement_hook(activations: torch.Tensor, hook: Any):\n",
    "                activations = cache[f'blocks.{layer}.hook_h.{seq_len - 1}']\n",
    "                return activations\n",
    "        return replacement_hook\n",
    "    \n",
    "    for pos in range(1, 5):\n",
    "        fwd_hooks = []\n",
    "        for l in range(model.cfg.n_layers):\n",
    "            hook_name = f'blocks.{l}.hook_h.{seq_len - 1}'\n",
    "            fwd_hooks.append((hook_name, generate_replacement_hook(l, patch_layer=specific_patch_layer, cache=cache, patch_cache=patch_cache, pos=pos)))\n",
    "        logits = model.run_with_hooks(induction_input, fwd_hooks=fwd_hooks, fast_ssm=False, fast_conv=True, warn_disabled_hooks=False)\n",
    "        real_logit_new = logits[:, second_A_index][torch.arange(logits.shape[0]), B_values]\n",
    "        fake_logit_new = logits[:, second_A_index][torch.arange(logits.shape[0]), patch_B_values]\n",
    "        sum_real_logit_new[pos - 1] += real_logit_new.sum()\n",
    "        sum_fake_logit_new[pos - 1] += fake_logit_new.sum()\n",
    "\n",
    "mean_real_logit_new = sum_real_logit_new / (N * batch_size)\n",
    "mean_fake_logit_new = sum_fake_logit_new / (N * batch_size)\n",
    "mean_real_logit_orig = sum_real_logit_orig / (N * batch_size)\n",
    "mean_fake_logit_orig = sum_fake_logit_orig / (N * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, -0.01, -9.13, 0.52, 0.16]\n"
     ]
    }
   ],
   "source": [
    "rounded_diff = [0.0] + [round(x.item(), 2) for x in mean_real_logit_new - mean_real_logit_orig]\n",
    "print(rounded_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba-sae)",
   "language": "python",
   "name": "mamba-sae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
